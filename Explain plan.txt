To find the execution plan of a Spark DataFrame, you can use the `explain()` method. This method displays the logical and physical execution plan generated by Spark's Catalyst optimizer for a given DataFrame. Here's how you can do it:

python
from pyspark.sql import SparkSession

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("ExecutionPlanDemo") \
    .getOrCreate()

# Create a DataFrame or load your DataFrame
# For example:
# df = spark.read.csv("path/to/your/file.csv")

# Apply transformations to the DataFrame
# For example:
# result_df = df.filter(df["column"] > 10).groupBy("column2").agg({"column3": "sum"})

# Display the execution plan
result_df.explain()




Replace `"path/to/your/file.csv"` with the actual path to your data file or DataFrame creation code if you're creating the DataFrame programmatically.

When you call `explain()` on a DataFrame (`result_df` in this example), Spark will print the logical and physical execution plan to the console. The execution plan provides insights into how Spark will execute the DataFrame transformations, including the applied optimizations, data shuffling, and stages of computation.

You can use the information from the execution plan to understand how Spark optimizes your queries and identify potential performance bottlenecks.