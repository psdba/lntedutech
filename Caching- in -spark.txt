Caching in Apache Spark allows you to persist DataFrames, RDDs, or Datasets in memory or on disk to avoid recomputation of intermediate results during multiple computations. Caching improves the performance of iterative or multiple computations on the same dataset by storing the data in memory or on disk, thereby reducing the need to recompute the dataset from the source data each time it is accessed.

Memory Cache:
- Memory caching stores the dataset in memory, making it readily accessible for subsequent computations. This can significantly improve performance when the dataset fits into memory and when it needs to be accessed multiple times.
- Use Case: In machine learning algorithms that involve iterative computations, caching intermediate datasets in memory can dramatically speed up the training process. For example, in iterative algorithms like Gradient Boosting or K-means clustering, caching the training dataset in memory between iterations can eliminate the need to read the dataset from disk repeatedly.

Disk Cache:
- Disk caching stores the dataset on disk, which can be useful when the dataset is too large to fit entirely in memory. While disk caching may not provide as much performance improvement as memory caching, it allows Spark to spill data to disk if memory is insufficient, thereby preventing out-of-memory errors.
- Use Case: When working with very large datasets that cannot fit into memory, disk caching enables Spark to leverage disk storage to store and retrieve intermediate results efficiently. For example, in batch processing jobs that involve processing terabytes of data, disk caching allows Spark to handle datasets larger than the available memory.

Demo:
```python
from pyspark.sql import SparkSession

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("CacheDemo") \
    .getOrCreate()

# Read data from a CSV file
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# Cache the DataFrame in memory
df.cache()

# Perform some transformations and actions on the DataFrame
result1 = df.filter(df["column"] > 10).count()
result2 = df.groupBy("category").agg({"value": "mean"}).show()

# Uncache the DataFrame
df.unpersist()

# Stop the SparkSession
spark.stop()
```
In this demo:
- We read a DataFrame `df` from a CSV file.
- We cache the DataFrame in memory using the `cache()` method.
- We perform some transformations and actions on the DataFrame, such as filtering and aggregating.
- After using the DataFrame, we unpersist it using the `unpersist()` method to release the cached data from memory.
- Finally, we stop the SparkSession.

By caching intermediate DataFrames in memory or on disk, Spark can reuse the computed results efficiently, leading to faster computations and improved overall performance.